2025-11-10 16:35:13,927 [INFO] Start training
2025-11-10 16:35:15,746 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2025-11-10 16:35:15,747 [INFO] Loaded 51189 records for train split from the dataset.
batch sizes [[1]]
llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.weight
llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.weight
llama_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight
llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.weight
llama_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.weight
llama_proj.weight
llama_proj.bias
2025-11-10 16:35:15,765 [INFO] number of trainable parameters: 56627200
/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/minigpt4/runners/runner_base.py:137: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
2025-11-10 16:35:15,766 [INFO] Start training epoch 0, 1000 iters per inner epoch.
/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/minigpt4/tasks/base_task.py:220: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp):
/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/minigpt4/models/base_model.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  return torch.cuda.amp.autocast(dtype=dtype)
/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Train: data epoch: [0]  [   0/1000]  eta: 0:09:24  lr: 0.000001  loss: 2.5887  time: 0.5649  data: 0.0000  max mem: 17744
Traceback (most recent call last):
  File "/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/train.py", line 104, in <module>
    main()
  File "/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/train.py", line 100, in main
    runner.train()
  File "/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/minigpt4/runners/runner_base.py", line 377, in train
    train_stats = self.train_epoch(cur_epoch)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/minigpt4/runners/runner_base.py", line 437, in train_epoch
    return self.task.train_epoch(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/minigpt4/tasks/base_task.py", line 116, in train_epoch
    return self._train_inner_loop(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/minigpt4/tasks/base_task.py", line 221, in _train_inner_loop
    loss = self.train_step(model=model, samples=samples)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/minigpt4/tasks/base_task.py", line 70, in train_step
    loss = model(samples)["loss"]
           ^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/minigpt4/models/minigpt_base.py", line 299, in forward
    outputs = self.llama_model(
              ^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/peft/peft_model.py", line 530, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/MiniGPT-Med/minigpt4/models/modeling_llama.py", line 65, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 922, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 672, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 406, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/g202516290/Documents/carbonateGPT/carbongpt/lib/python3.12/site-packages/torch/nn/functional.py", line 2135, in softmax
    ret = input.softmax(dim, dtype=dtype)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 19.58 GiB of which 101.94 MiB is free. Including non-PyTorch memory, this process has 18.04 GiB memory in use. Of the allocated memory 17.55 GiB is allocated by PyTorch, and 248.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
